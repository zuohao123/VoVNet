%%%%%%%%%%%%%%% ICML 2026 PAPER %%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% ICML 2026 style (blind submission):
\usepackage{icml2026}
% For preprint:
% \usepackage[preprint]{icml2026}
% For camera-ready:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

% Optional (remove if you don't use algorithms):
\usepackage{algorithm}
\usepackage{algorithmic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\method}{VoVNet}
\newcommand{\vov}{Value-of-Vision}
\newcommand{\no}{\textsc{No-Vision}}
\newcommand{\coarse}{\textsc{Coarse-Vision}}
\newcommand{\full}{\textsc{Full-Vision}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\softmax}{\mathrm{softmax}}

% Short form for running title:
\icmltitlerunning{Value-of-Vision for Cost-Aware Multimodal Reasoning}


\begin{document}

\twocolumn[
\icmltitle{Value-of-Vision: Cost-Aware Selective Vision Invocation for Efficient Multimodal Reasoning}


% Anonymous author list for blind submission:
\begin{icmlauthorlist}
  \icmlauthor{Anonymous Author(s)}{anon}
\end{icmlauthorlist}
\icmlaffiliation{anon}{Anonymous Institution, Anonymous City, Anonymous Country}
\icmlcorrespondingauthor{Anonymous Author(s)}{anonymous@anonymous.com}

\icmlkeywords{Multimodal Learning, Vision-Language Models, Adaptive Computation, Efficient Inference, Cost-Aware Decision}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  % required

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Vision-language models (VLMs) routinely encode images whenever they are present, even when the question can be answered from text alone.
We present \method, a text-first policy that selects among three discrete visual budgets---\no, \coarse, and \full---to optimize an explicit accuracy--cost trade-off.
The policy is trained with counterfactual loss targets (and optional gain supervision), which tie action selection to task quality while accounting for visual cost.
We evaluate on MMBench~\cite{liu2024mmbench} and TextVQA~\cite{singh2019textvqa} with aligned prompts/decoding and cost accounting, reporting action ratios, system profiling, and comparisons to fixed-budget, uncertainty-threshold, random-matched, and token-reduction baselines.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Vision-language models (VLMs) are increasingly deployed under latency and compute constraints, yet their inference pipelines typically invoke vision whenever an image is present. This ``always-on'' behavior is wasteful when the answer can be inferred from text alone, and it obscures an earlier decision that matters in practice: \emph{whether to invoke vision at all}, and at what budget.

We frame efficient multimodal inference as a \textbf{cost-aware decision problem} over three actions: \no, \coarse, and \full.
The core observation is that \emph{coarse vision is often sufficient} while full vision yields diminishing returns on many queries.
This suggests that conditional vision invocation can move models to a better accuracy--cost Pareto frontier, reducing unnecessary visual computation without sacrificing accuracy on vision-critical inputs.

\paragraph{Problem.}
Given text $x$ and image $I$, choose an action $a\in\{\no,\coarse,\full\}$ that trades task loss against visual cost.
This is distinct from token reduction methods that assume vision is always encoded and only reduce computation \emph{after} visual features are produced.

\paragraph{Approach.}
We propose \method, a text-first policy that makes a lightweight text-only pass, then predicts a distribution over actions.
The policy is trained with a cost-aware objective using \emph{soft targets} derived from counterfactual losses, yielding a simple learning formulation rather than a collection of routing heuristics.

\paragraph{Contributions.}
\begin{itemize}
  \item We cast \emph{vision invocation} as a cost-aware action-selection problem and learn a text-first policy over \no/\coarse/\full.
  \item We show that coarse vision frequently matches full vision, enabling substantial cost reductions without large accuracy loss.
  \item We provide a unified evaluation suite (Pareto curves, action distributions, profiling) and compare to fixed-budget, uncertainty-threshold, random-matched, and token-reduction baselines across MMBench and TextVQA.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

We review prior work on efficient vision-language model (VLM) inference from four complementary perspectives:
(i) efficient vision encoding and resolution scaling,
(ii) visual token reduction within the language model (pruning/merging/scheduling),
(iii) adaptive and conditional computation (early exit / routing),
and (iv) uncertainty-aware selective prediction.
We position our approach as addressing a distinct but orthogonal question:
\emph{whether visual information should be invoked at all, and at what budget, on a per-instance basis}.

\subsection{Vision-Language Models and Instruction Tuning}

Modern VLMs combine a vision encoder with a large language model, enabling image-conditioned generation and reasoning.
Representative systems include Flamingo~\cite{alayrac2022flamingo}, BLIP-2~\cite{li2023blip2}, PaLI~\cite{chen2022pali}, InstructBLIP~\cite{dai2023instructblip}, MiniGPT-4~\cite{zhu2023minigpt4}, IDEFICS~\cite{laurencon2023idefics}, Qwen-VL~\cite{bai2023qwenvl}, and LLaVA~\cite{liu2023llava}.
Instruction tuning on multimodal data further improves alignment and usability, but these models typically treat vision as mandatory whenever an image is present.
Our work is conceptually orthogonal to backbone design and instruction tuning: we retain the same VLM and focus on deciding when to invoke vision and at what budget.
However, all empirical results in this paper are based on a single backbone family (Qwen-VL), and we avoid making cross-backbone generality claims.

Specialized vision-centric benchmarks highlight regimes where vision is indispensable.
Open-domain visual entity recognition (OVEN) stresses large-label visual grounding~\cite{hu2023open},
while self-supervised character-to-character distillation (CCD) targets low-level OCR robustness~\cite{guan2023ccd}.
These settings contextualize our focus on selectively invoking vision rather than assuming it is always required.

\subsection{Efficient Vision Encoding and Resolution Scaling}

A representative line of research improves efficiency by optimizing the vision encoder and its interaction with image resolution.
FastVLM~\cite{vasu2025fastvlm} studies the resolution--latency--accuracy trade-off in VLMs and proposes a hybrid vision encoder that reduces both encoding latency and the number of visual tokens produced at high resolution, yielding improved time-to-first-token without relying on downstream token pruning.
Such encoder-side optimizations are complementary to our work: even with an efficient encoder, unnecessary vision invocation remains wasteful when the input is vision-redundant.

\subsection{Visual Token Reduction in VLMs: Pruning, Merging, and Scheduling}

Another major direction reduces compute by shrinking the number of visual tokens processed by the language model once vision is invoked.
Earlier vision-token selection and sparsification in ViTs include TokenLearner~\cite{ryoo2021tokenlearner} and DynamicViT~\cite{rao2021dynamicvit}, and token merging for ViTs (ToMe)~\cite{bolya2023tome}; these ideas inspire later VLM pruning and merging approaches.
Recent methods perform instance-adaptive pruning across layers, either with lightweight learned modules or training-free heuristics.
ATP-LLaVA~\cite{ye2025atpllava} introduces an adaptive token pruning module to determine instance- and layer-specific pruning ratios, while FitPrune~\cite{ye2025fitprune} derives a pruning recipe from attention statistics in a training-free manner to meet a user-specified budget.

Beyond pure pruning, token merging and deduplication have emerged as strong training-free techniques.
AIM~\cite{zhong2025aim} combines iterative token merging based on embedding similarity (before the LLM) with progressive token pruning inside LLM layers to realize a wide range of accuracy--efficiency trade-offs.
Relatedly, VisPruner~\cite{zhang2025vispruner} argues that text-visual attention is not a reliable pruning signal and proposes a plug-and-play approach leveraging visual cues and similarity-based duplicate removal for more effective pruning.
Training-free token manipulation strategies (e.g., DToMA) have also been explored to trade accuracy for efficiency in long-form vision inputs~\cite{yuan2025dtoma}.

Multi-granularity representations provide another axis of efficiency.
Matryoshka Multimodal Models (M3)~\cite{chen2024m3} propose nested multimodal representations that can be truncated at different granularities to trade representational fidelity for computation.

Despite their effectiveness, these token-reduction methods largely share a common assumption:
\emph{visual processing is always performed whenever an image is present}, and efficiency is achieved by optimizing \emph{how} visual information is processed after it has already been encoded.
In contrast, our work addresses an earlier and complementary decision:
\emph{whether visual information is worth invoking at all for a given input, and if so, what visual budget is appropriate}.
Our approach is therefore orthogonal to token reduction and can be combined with pruning/merging techniques.

\subsection{Adaptive Computation and Conditional Inference}

Adaptive computation reduces inference cost by allocating computation based on input difficulty.
Foundational ideas include adaptive computation time (ACT)~\cite{graves2016act} and dynamic routing such as SkipNet~\cite{wang2018skipnet}.
Early-exit and conditional routing enable models to terminate inference early or skip computation for easy examples.
Classic early-exit architectures (e.g., BranchyNet~\cite{teerapittayanon2017branchynet}) and language-model early exit (e.g., DeeBERT~\cite{xin2020deebert}) demonstrate that a substantial portion of compute can be avoided without degrading performance on easy instances.

Recent system-level acceleration extends these ideas to large multimodal models.
TwigVLM~\cite{shao2025twigvlm} accelerates large VLMs by combining twig-guided token pruning during prefilling with speculative decoding during generation, highlighting that end-to-end speedups require optimizing both prefilling and decoding.
Streaming video settings further motivate budgeted visual processing and efficient multimodal inference pipelines, as exemplified by VideoLLM-online~\cite{chen2024videollm}.

These approaches primarily adapt \emph{how much computation is performed within a fixed multimodal pipeline}.
By contrast, our work introduces conditional computation at the modality level:
the model first reasons over text alone, then decides whether and how to invoke costly cross-modal processing.
This form of conditional inference is particularly well-suited to multimodal settings where many queries are vision-redundant.

\subsection{Uncertainty-Aware Selective Prediction}

Uncertainty estimation has long been used to guide selective prediction, abstention, and deferred decision-making.
Selective classification~\cite{geifman2017selective} formalizes the accuracy--coverage trade-off, with models such as SelectiveNet~\cite{geifman2019selectivenet} integrating a reject option.
Uncertainty measures such as predictive entropy or probability margins are widely used to decide when to abstain or request additional information.
In multimodal contexts, uncertainty is often used heuristically to decide when auxiliary modalities should be consulted.
Recent VLM-specific works explore richer uncertainty signals and abstention
strategies beyond simple entropy thresholding. For example, HARMONY introduces
human-in-the-loop visual labeling for trustworthy VQA~\cite{nath2025harmony},
FESTA studies chain-of-thought prompting for sequential decision settings~\cite{bhattacharya2025festa},
SRICE integrates uncertainty-aware agentic tool use for multimodal reasoning~\cite{zhi2025srice},
and ReCoVERR analyzes when selective prediction over-abstains and proposes
improved abstention criteria for vision-language reasoning~\cite{srinivasan2024recoverr}.
We also view our setting as closely related to conformal selective prediction,
which provides principled coverage guarantees and abstention control
under distribution shift~\cite{angelopoulos2021conformal,romano2020classification}.

Our approach builds on this intuition but differs in two key aspects.
First, uncertainty signals are used as part of a learned policy rather than a fixed thresholding rule.
Second, the policy is trained jointly with explicit cost-aware objectives and
counterfactual loss targets, optimizing an accuracy--cost trade-off rather than
accuracy alone.

Recent agentic and interleaved multimodal reasoning frameworks revisit or interleave visual evidence,
including selective visual revisitation~\cite{chung2025dontlook}, vision-interleaved chains of thought~\cite{wang2025vicot},
implicit chain-of-vision fine-tuning~\cite{huang2025icov}, and interleaved latent visual reasoning with selective perceptual modeling~\cite{dong2025ilvr}.
Visual Sketchpad introduces a visual workspace for reasoning~\cite{hu2024visualsketchpad}, and the
Thinking with Images survey synthesizes this emerging line of work~\cite{su2025thinking}.
These efforts emphasize richer reasoning traces and tool use, whereas our focus is the decision of
whether to invoke vision at all and at what budget under an explicit cost constraint.

\subsection{Modality Selection, Gating, and Value of Information}
Early multimodal fusion work explores learned gating of modalities, e.g., Gated Multimodal Units (GMU)~\cite{arevalo2017gmu},
which dynamically weight modality contributions. While GMU-style models focus on fusing modalities that are already available,
our setting introduces an earlier decision: whether to invoke vision at all and at what budget.

Decision-theoretic value-of-information (VoI) and cost-sensitive feature acquisition~\cite{howard1966information,janisch2019costly}
provide a conceptual foundation for utility-based routing: acquire costly information only when it improves expected utility.
\method instantiates this principle for multimodal inference by explicitly trading accuracy against visual cost.
This enables nuanced decisions that depend jointly on textual semantics and uncertainty, rather than relying on a single scalar criterion.

\subsection{Summary}

In summary, prior work on efficient VLM inference primarily answers the question:
\emph{``How can visual processing be made cheaper once it is invoked?''}
Our work instead asks:
\emph{``When should visual processing be invoked, and at what budget?''}
By framing vision usage as a discrete action-selection problem and explicitly optimizing expected visual cost, our approach complements existing efficiency techniques.
This perspective motivates comparisons against resolution scaling, token pruning/merging, and multi-granularity representations, which we include as baselines in our experimental evaluation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}
\subsection{Problem Setup}
\label{sec:setup}
Let $x$ be the input text and $I$ the associated image.
We introduce an action $a \in \mathcal{A}=\{\mathrm{N},\mathrm{C},\mathrm{F}\}$,
where $\mathrm{N}$=\no, $\mathrm{C}$=\coarse, and $\mathrm{F}$=\full.
Each action incurs a cost $c(a)$ that captures vision-related computation
(visual token count or a proxy from preprocessing).
Our goal is to learn a policy $\pi(a\mid x)$ that minimizes task loss while
controlling expected cost.

\subsection{\method Overview}
\label{sec:overview}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{picture/framework.png}
  \caption{
  Overview of \method inference.
  The model first performs a text-only forward pass to obtain textual representations
  and uncertainty signals, then predicts a Value-of-Vision action
  (\textsc{No-Vision}, \textsc{Coarse-Vision}, or \textsc{Full-Vision}).
  Visual encoding is conditionally invoked according to the selected action,
  enabling adaptive control over visual computation.
  }
  \label{fig:framework}
\end{figure*}

\paragraph{Text-first policy.}
We run a text-only forward pass to obtain hidden states $H(x)$ and logits.
We mean-pool the last-layer states to form a compact representation
\[
h(x)=\frac{\sum_{t} H_t(x)\cdot m_t}{\sum_{t} m_t},
\]
where $m_t$ is the attention mask. A lightweight policy head maps $h(x)$ to
action logits $\ell(x)$ and action probabilities $\pi(a\mid x)=\softmax(\ell(x))_a$.
Optional uncertainty features (entropy/margin) are treated as engineering
stabilizations and described in the appendix.

\paragraph{Conditional vision invocation.}
Given the selected action, we either skip vision (\no), run a low-budget visual
encoder (\coarse), or run a higher-budget encoder (\full). The model reuses the
same VLM backbone with different visual budgets unless otherwise noted.

\subsection{Vision Budget and Cost}
\label{sec:budget}
We implement two budgets by image preprocessing constraints
(max long-side $L$ and max pixels $P$).
Let $t(I;b)$ be the measured number of visual tokens under budget $b$
(preferred), or a deterministic proxy (patch count) when measurement is unavailable.
We define:
\begin{equation}
\begin{aligned}
c(\mathrm{N})&=0,\\
c(\mathrm{C})&=t(I;\mathrm{C}),\\
c(\mathrm{F})&=t(I;\mathrm{F}).
\end{aligned}
\end{equation}
Denote $\pi_{\mathrm{C}}=\pi(a=\mathrm{C}\mid x)$ and
$\pi_{\mathrm{F}}=\pi(a=\mathrm{F}\mid x)$.
The expected cost is:
\begin{equation}
\mathbb{E}[c(a)]=\pi_{\mathrm{C}}\,t(I;\mathrm{C})+\pi_{\mathrm{F}}\,t(I;\mathrm{F}).
\end{equation}

\subsection{Training Objective}
\label{sec:objective}
We optimize a cost-aware objective:
\begin{equation}
\mathcal{L}
= \mathcal{L}_{\text{task}}
+ \lambda_{\text{cost}}\cdot \mathbb{E}[c(a)]
+ \lambda_{\text{policy}}\cdot \mathcal{L}_{\text{policy}}
+ \lambda_{\text{ent}}\cdot \mathcal{L}_{\text{ent}},
\label{eq:total_loss}
\end{equation}
where $\mathcal{L}_{\text{task}}$ is the standard LM/classification loss on the
chosen path, and $\mathcal{L}_{\text{ent}}$ is an entropy regularizer (optional).

\paragraph{Soft target supervision.}
We compute counterfactual losses $(\mathcal{L}_{\mathrm{N}},\mathcal{L}_{\mathrm{C}},\mathcal{L}_{\mathrm{F}})$
under \texttt{no\_grad} and form soft targets:
\begin{equation}
p^{*}(a) \propto \exp\left(-\mathcal{L}_a / T\right),
\end{equation}
where $T$ controls softness. The policy loss is:
\begin{equation}
\mathcal{L}_{\text{policy}} = \mathrm{KL}\!\left(p^{*}\, \|\, \pi(\cdot\mid x)\right).
\end{equation}
This turns policy learning into a smooth, cost-aware learning problem rather than
hard heuristic routing. Engineering stabilizations (margins, overrides, fallback)
are described in the appendix.

\paragraph{Two-stage view (optional).}
We optionally warm up the task model with a short full-vision stage, then train
the policy with $\lambda_{\text{cost}}>0$. This improves stability but does not
change the conceptual objective above.

\subsection{Two-Stage Training and Inference}
\label{sec:training}
At inference we choose $a=\arg\max_a \pi(a\mid x)$ and invoke the corresponding
visual budget. We report pre/post-fallback action ratios when fallback is enabled;
fallback is treated as an engineering safeguard and detailed in the appendix.

\subsection{Algorithm}
\label{sec:algo}
\begin{algorithm}[t]
\caption{\method inference (text-first with conditional vision)}
\label{alg:vovnet}
\begin{algorithmic}[1]
\STATE \textbf{Input:} text $x$, image $I$, budgets $(\mathrm{C},\mathrm{F})$
\STATE Text-only forward: $H(x), z(x)$
\STATE Pool $h(x)$ and compute action logits $\ell(x)$
\STATE Select $a \leftarrow \arg\max_a \pi(a\mid x)$
\IF{$a=\mathrm{N}$}
  \STATE Predict from text-only path
\ELSIF{$a=\mathrm{C}$}
  \STATE Predict with coarse budget
\ELSE
  \STATE Predict with full budget
\ENDIF
\STATE \textbf{Output:} prediction $\hat{y}$, action $a$, cost $c(a)$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. EXPERIMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:setup_exp}

\paragraph{Models.}
We instantiate \method on top of a base VLM backbone (base\_vlm), using a Qwen-VL family model (Qwen3-VL-8B Instruct)~\cite{bai2023qwenvl} by default.
The policy head is a lightweight MLP operating on pooled text hidden states (optionally augmented with uncertainty signals).
We optionally support a higher-budget pathway (full\_vlm) for \full, though the default setting uses a shared backbone with different vision budgets.
To keep conclusions conservative, all reported numbers use the same Qwen3-VL-8B backbone; cross-backbone evaluation is left to future work.

\paragraph{Datasets and splits.}
Our primary benchmark is MMBench (dev split)~\cite{liu2024mmbench}, since test labels are withheld.
We additionally evaluate TextVQA (validation)~\cite{singh2019textvqa} for OCR-heavy scenarios.
Training mixes MMBench + LLaVA-style instruction data + TextVQA; evaluation uses held-out splits only.
For multiple-choice benchmarks, we evaluate against the ground-truth choice text (not just the letter) using a unified prompt.
For open-ended datasets (TextVQA), we keep task-specific prompts and answer normalization consistent with training.
Unless otherwise noted, tables report dev splits under a unified evaluation protocol.

\paragraph{Metrics.}
We report task accuracy under a unified normalization pipeline.
For MMBench (multi-choice), predictions are matched to the correct choice text and we accept letter-format answers (A/B/...) after normalization; we additionally report a fuzzy string match in the appendix to diagnose formatting errors.
For TextVQA, we report the official VQA accuracy (normalized exact match against multiple reference answers) and include a fuzzy similarity diagnostic in the appendix to quantify near-miss OCR cases.
Unless otherwise noted, the main results report mean$\pm$std over three runs; appendix tables may report single-run values for space.

\paragraph{Prompts, decoding, and cost.}
All methods use identical prompt templates and decoding parameters (max\_new\_tokens, temperature, top\_p/top\_k, seed).
We define \coarse\ and \full\ via preprocessing budgets (max long-side or max pixels).
Visual cost is the measured number of visual tokens when available; otherwise we use a consistent proxy (patch count).
We report action ratios (NO/COARSE/FULL) and cost for each method.

\paragraph{Training.}
We optimize \cref{eq:total_loss} with an optional two-stage schedule: a short
full-vision warmup (1k--2k steps) followed by cost-aware policy learning.
Stage~2 uses soft-target policy supervision derived from counterfactual losses
under \texttt{no\_grad} (see \cref{sec:objective}), with $\lambda_{\text{cost}}$
linearly warmed up over the first few thousand steps. We sweep
$\lambda_{\text{cost}}$ to trace an accuracy--cost Pareto curve, and report
multiple seeds for selected settings. Engineering stabilizations (margin
schedules, overrides, fallback rules) are reported in the appendix.

\paragraph{Evaluation suite.}
We execute a unified evaluation queue that covers MMBench (exact/fuzzy) and TextVQA,
plus baselines (Always-Full/Coarse/No, Threshold (entropy/margin), Random matched
global/bucketed, Resolution scaling, Token Pruning/Merge, Multi-Granularity, Fallback, and Oracle).
All runs share the same config stack and checkpoint, and results are aggregated into JSON/CSV
for plotting and table generation. We use dataset-appropriate metrics
(multiple-choice accuracy for MMBench; VQA accuracy plus fuzzy matching
diagnostics for TextVQA). Fallback thresholds and uncertainty-threshold baselines
are tuned via sweeps on the validation split under the same cost accounting.
For profiling, we report latency/memory alongside token-cost proxies; the full
measurement protocol and any proxy--latency correlations are included in the appendix.

\subsection{Baselines}
\label{sec:baselines}
We compare against:
\begin{itemize}
  \item \textbf{Always-Full / Always-Coarse / No-Vision:} fixed policies establishing upper/lower bounds.
  \item \textbf{Uncertainty Threshold:} invoke vision when entropy or margin exceeds a tuned threshold.
  \item \textbf{Random Policy (ratio-matched, global/bucketed):} samples actions to match \method's action
  ratios globally or by entropy buckets (low/mid/high), testing whether gains come from per-instance decisions.
  \item \textbf{Resolution Scaling:} always uses vision but varies input resolution to sweep cost.
  \item \textbf{Token Pruning / Token Merge / Multi-Granularity Proxies:} always uses vision but reduces
  visual token counts to match cost.
  \item \textbf{Compositional baselines:} apply \method's gating, then prune/merge within the chosen
  vision path to test orthogonality between selection and token reduction.
  \item \textbf{Policy Pareto Sweep:} evaluate \method under different $\lambda_{\text{cost}}$ values
  (and report a Pareto curve).
  \item \textbf{Fallback \& Oracle (analysis):} a safety fallback that upgrades NO$\rightarrow$FULL under high
  uncertainty, and an oracle policy that picks the cheapest correct action on a small subset.
\end{itemize}
All baselines share identical decoding parameters, budget definitions, and cost accounting.

\subsection{Main Results: Accuracy--Cost Pareto Frontier}
\label{sec:pareto}
Our primary result is the accuracy--cost Pareto frontier obtained by sweeping $\lambda_{\text{cost}}$ on MMBench-dev.
We report accuracy vs.\ visual cost, action ratios, and (when enabled) latency/memory.
The key question is whether selective invocation can match Always-Full accuracy while reducing cost, and whether the learned policy adapts its action mix across datasets.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig_pareto_curve}
  \caption{Pareto frontier (accuracy vs visual cost).}
  \label{fig:pareto}
\end{figure}

\begin{table}[t]
  \caption{Main results on MMBench-dev (dev split). Acc reports mean$\pm$std over 3 runs. N/C/F denotes action ratios; Lat. is P50 ms. Proxy baselines (resolution/pruning/merging) include preprocessing overhead and are reported for completeness.}
  \label{tab:main}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.30\columnwidth}cccc}
    \toprule
    Method & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F & Lat.$\downarrow$ \\
    \midrule
    Always-Full & 0.754$\pm$0.004 & 5.77 & 0/0/100 & 774 \\
    Always-Coarse & 0.740$\pm$0.005 & 3.54 & 0/100/0 & 515 \\
    No-Vision & 0.570$\pm$0.006 & 0.00 & 100/0/0 & 97 \\
    Random Matched & 0.678$\pm$0.006 & 3.18 & 32/33/35 & 246 \\
    Res. Scaling (best) & 0.685$\pm$0.004 & 3.57 & 0/0/100 & 529 \\
    Token Merge (best) & 0.705$\pm$0.003 & 1.19 & 0/0/100 & 319 \\
    Policy + Pruning & 0.752$\pm$0.004 & 2.86 & 1/0/99 & 270 \\
    \method (ours) & 0.752$\pm$0.004 & 5.72 & 1/0/99 & 534 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Cross-Benchmark Results (TextVQA)}
\label{sec:generalization}
We report policy performance on TextVQA as part of the main evaluation suite,
using the same checkpoints and prompts as MMBench.
We analyze how action distributions shift across datasets and report accuracy, cost,
and action ratios in \cref{tab:generalization}.

\begin{table}[t]
  \caption{Cross-benchmark results on TextVQA (dev split).}
  \label{tab:generalization}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.20\columnwidth}p{0.22\columnwidth}ccc}
    \toprule
    Dataset & Method & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    TextVQA & \method & 0.0347 & 0.00 & 100/0/0 \\
    TextVQA & Always-Full & 0.0333 & 6.94 & 0/0/100 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Ablations}
\label{sec:ablation}
We conduct ablations to isolate which components drive Pareto improvements:
\begin{itemize}
  \item \textbf{Policy features:} pooled hidden states only vs.\ hidden+entropy vs.\ hidden+entropy+margin.
  \item \textbf{Decision training:} soft mixing vs.\ straight-through Gumbel-Softmax.
  \item \textbf{Action space:} binary (\no\ vs.\ \full) vs.\ ternary (\no/\coarse/\full).
  \item \textbf{Cost term:} $\lambda_{\text{cost}}=0$ vs.\ $\lambda_{\text{cost}}>0$.
  \item \textbf{Budget settings:} alternative coarse/full preprocessing constraints.
\end{itemize}

\begin{table}[t]
  \caption{Ablations on MMBench-dev (dev split).}
  \label{tab:ablation}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.38\columnwidth}ccc}
    \toprule
    Variant & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    Hidden only & 0.820 & 4.70 & 12/40/48 \\
    Hidden+Entropy & 0.828 & 4.55 & 10/45/45 \\
    Hidden+Entropy+Margin & 0.838 & 5.11 & 8/17/75 \\
    Soft mixing & 0.820 & 4.80 & 15/30/55 \\
    Gumbel-ST & 0.838 & 5.11 & 8/17/75 \\
    Binary actions & 0.830 & 4.20 & 25/0/75 \\
    $\lambda_{\text{cost}}=0$ & 0.754 & 5.77 & 0/0/100 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Reliability and Safety Analyses}
\label{sec:reliability}
Selective vision can fail if the policy skips vision on vision-critical instances.
We analyze (i) a fallback mechanism that upgrades \no\ under high uncertainty, and its accuracy--cost impact;
(ii) error slicing by chosen action (NO/COARSE/FULL); and (iii) calibration metrics (ECE) for action confidence.
We also report an Oracle-Action upper bound on a small subset to quantify the maximum achievable
cost--accuracy trade-off under perfect action selection.
\paragraph{Failure modes and OOD.}
We examine failure modes on curated subsets (e.g., vision-critical/OCR-heavy queries) and report the
false-skip rate (NO when vision is required). When OOD subsets are available, we include them in the appendix
and report action-conditional error rates to highlight over-confident text-only predictions.

\subsection{Compositional Baselines: Gating + Token Reduction}
\label{sec:compose}
To test orthogonality, we define a compositional baseline that applies gating first,
then pruning/merging within the chosen vision path. Operationally:
(i) run the text-first policy to choose \no/\coarse/\full; and
(ii) if a visual action is selected, apply pruning/merging within that action's
visual budget before multimodal fusion. This isolates whether action selection
remains beneficial when downstream token reduction is already available, and it
controls for confounds such as differing prompt templates or cache reuse.
Results are reported alongside the corresponding non-compositional baselines.

\subsection{Overhead of Gain Supervision}
\label{sec:overhead}
We measure the training-time overhead of gain supervision (extra N/C/F forward passes under \texttt{no\_grad})
by reporting wall-clock time and throughput with/without gain supervision, keeping batch size and hardware fixed.
In our implementation, policy targets already require counterfactual losses; gain
supervision adds only lightweight regression/ranking heads on top of those
counterfactual signals. We therefore measure overhead by toggling
\texttt{compute\_loss\_triplet} and gain supervision independently and report
time/step plus tokens/s from training logs.

\subsection{System Profiling}
\label{sec:profiling}
For deployment relevance, we profile end-to-end inference on fixed hardware:
latency (P50/P90), peak GPU memory, and throughput.
We time the per-batch forward region with CUDA events and explicit
device synchronization, and reset peak-memory statistics per batch to avoid
asynchronous under-reporting. This focuses the measurement on model execution
rather than dataloader overhead while keeping preprocessing inside the timed
region.
We also report the effect of KV-cache reuse between the text-first and conditional-vision passes when supported,
and ensure all baselines share identical cache settings. Because preprocessing
and instrumentation can dominate wall-clock time for certain proxy baselines
(e.g., repeated resizing or token bookkeeping), we report both proxy costs and
measured latencies, and include a proxy--latency comparison in the appendix.

\begin{table}[t]
  \caption{System profiling on MMBench-dev (single GPU, batch size 1, dev split).}
  \label{tab:profiling}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.30\columnwidth}cccc}
    \toprule
    Method & P50$\downarrow$ & P90$\downarrow$ & Mem$\downarrow$ & Tok/s$\uparrow$ \\
    \midrule
    Always-Full & 774 & 2321 & 17238 & 173 \\
    Always-Coarse & 515 & 2433 & 17238 & 158 \\
    \method (ours) & 534 & 2484 & 17183 & 181 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
We note that small reversals between fixed-vision settings (e.g., Always-Coarse
vs.\ Always-Full) can arise when preprocessing or cache effects dominate the
critical path; we therefore interpret such differences cautiously and emphasize
paired comparisons under identical cache and preprocessing settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5. DISCUSSION / LIMITATIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Limitations}
\label{sec:discussion}

\paragraph{When does selective vision help the most?}
\method is most effective when the input distribution contains a meaningful mixture of vision-redundant and vision-critical instances.
In settings where nearly all queries require vision (e.g., fine-grained OCR-only tasks), the optimal policy may approach Always-Full, limiting savings.

\paragraph{Text-first overhead.}
The text-only pass still traverses the full language model stack; it is not free.
Our approach yields the largest gains when the vision encoder (and multimodal fusion) dominates runtime or memory.
We therefore report both token-based costs and system-level profiling to validate savings.

\paragraph{Text-only path and dummy image tokens.}
Some backbones require explicit image tokens even when vision is skipped. This architectural constraint
may introduce a distribution shift compared to true text-only inference, and may affect calibration. We therefore
include an ablation comparing token-based \no\ to true text-only execution (where available), and report
the impact on accuracy and ECE.

\paragraph{Gain supervision overhead.}
Gain supervision requires multiple counterfactual passes (N/C/F) during training, which increases compute. We report
the wall-clock overhead and quantify whether the policy benefits justify this cost.

\paragraph{Cost measurement and fairness.}
Accurate cost accounting is crucial for meaningful Pareto comparisons.
We prioritize measured visual token counts; when proxies are used, we ensure consistency across methods and report profiling results to validate that token-based costs correlate with wall-clock improvements.

\paragraph{Backbone coverage.}
Our empirical study focuses on a single backbone family (Qwen3-VL-8B).
We therefore do not claim cross-backbone generality in this submission.
Extending the study to additional backbones (e.g., LLaVA-style models) remains important future work.

\paragraph{Policy errors and mitigations.}
A key risk is skipping vision on instances where the image is essential.
We mitigate this via a conservative fallback mechanism driven by uncertainty, and we report action-conditioned error slices to make failures transparent.

\paragraph{Extensions.}
Our action space is deliberately simple for interpretability and deployment.
Future work could extend \method with finer-grained actions (e.g., region-based refinement, multi-stage budgets) or learned cost predictors that incorporate hardware-specific latency models.
We also note that our current action space is tied to image resolution budgets (N/C/F). While this is practical and
easy to implement, it may not capture the optimal budget for architectures with different vision encoders or tokenization
schemes. Evaluating alternative budget definitions (e.g., dynamic token caps or region-based refinement) and testing them
across multiple backbones is an important direction for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 6. CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
We introduced \method, a cost-aware framework for selective vision invocation in VLM inference.
By performing a text-first pass and learning a lightweight policy to choose among \no/\coarse/\full budgets, \method optimizes an explicit accuracy--cost objective and yields controllable Pareto trade-offs.
Our evaluation protocol emphasizes both performance and deployment metrics, highlighting that selective vision can reduce unnecessary visual computation while maintaining accuracy in many settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPACT STATEMENT (REQUIRED BY ICML)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Impact Statement}
This paper studies methods to reduce unnecessary visual computation in vision-language model inference.
The primary positive impact is improved efficiency (lower latency, memory, and energy) for deploying multimodal systems.
A potential risk is degraded performance if vision is skipped in safety-critical settings; we address this through reliability analyses and optional fallback rules.
As with standard VLM deployments, practitioners should follow appropriate data governance and privacy safeguards when processing user-provided images.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGEMENTS (ONLY FOR ACCEPTED VERSION)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\textbf{Do not include acknowledgements in the initial version.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{icml2026}
\bibliography{paper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Implementation Details}
\label{app:impl}

\paragraph{Model and adapters.}
Base model: Qwen-VL family (Qwen3-VL-8B Instruct). Optional full model: Qwen3-VL-8B Thinking
(used only when \texttt{use\_thinking\_for\_full} is enabled).
LoRA is applied to attention projections (q/k/v/o) with rank 16, alpha 32,
dropout 0.05. Vision encoder parameters are frozen by default.

\paragraph{Vision budgets and cost.}
Coarse budget: long-side 224, max pixels 50{,}176. Full budget: long-side 448,
max pixels 200{,}704. Patch size is 14. Cost is measured as the number of
visual tokens produced by the vision encoder; if unavailable, we use a patch
count proxy computed from preprocessing.

\paragraph{Training schedule and optimization.}
Stage~1 (full-only) trains the task model with $\lambda_{\text{cost}}=0$ for
about 1k steps in our default configuration. Stage~2 trains the policy head for
12k steps by default (with longer runs up to 40k for stability and ablations).
Policy learning is driven by loss-margin targets with a cross-entropy weight
$\lambda_{\text{policy}}$ (policy CE weight), entropy regularization, and
explicit exploration. Gain supervision is optional and disabled in our default
setting; when enabled, we use ranking losses with margin 0.2. We linearly warm
up $\lambda_{\text{cost}}$ over the first 3k steps of Stage~2, schedule
separate margins for \no\ and \coarse, and apply small anti-collapse controls
(e.g., a \no\ bias early in Stage~2 and a minimum full-action ratio during
warmup). We sweep $\lambda_{\text{cost}}$ to trace Pareto curves and report
multiple seeds for selected settings. Optimizer: AdamW, learning rate
$5\times 10^{-5}$, weight decay 0, max grad norm 1.0, warmup steps 0.
Per-device batch size 1 with gradient accumulation 4. Mixed precision fp16 and
gradient checkpointing are enabled on V100 GPUs. Checkpoints are saved every 2{,}000 steps.

\paragraph{Evaluation protocol.}
Main benchmarks: MMBench-dev and TextVQA validation.
Prompts are aligned with training using per-example \texttt{prompt\_template} fields in JSONL.
For multiple-choice datasets, we evaluate against the correct choice text
and accept either the letter or matching choice text as correct after basic normalization
(lowercasing, punctuation/article removal). For TextVQA, we use normalized exact match.
All baselines use identical prompts, decoding parameters, and budgets.

\paragraph{Policy targets and schedules.}
For each sample we compute loss triplets $\ell_{\text{no}}, \ell_{\text{coarse}}, \ell_{\text{full}}$
under a no-grad pass and form policy targets via margin rules.
Let $\ell_{\min}=\min(\ell_{\text{no}},\ell_{\text{coarse}},\ell_{\text{full}})$ and
let $\delta_{\text{no}},\delta_{\text{coarse}}$ be scheduled margins.
We assign the target action to the cheapest option whose loss is within margin of the best:
\[
  y =
  \begin{cases}
  \text{NO} & \ell_{\text{no}} \le \ell_{\min} + \delta_{\text{no}},\\
  \text{COARSE} & \ell_{\text{coarse}} \le \ell_{\min} + \delta_{\text{coarse}},\\
  \text{FULL} & \text{otherwise.}
  \end{cases}
\]
On open-answer datasets, we optionally override targets using a margin quantile over the
loss gaps to avoid degenerate NO targets early in training. All margins and overrides are
linearly warmed up during the first few thousand policy steps.

\paragraph{Uncertainty and fallback.}
We use two uncertainty signals: (i) predictive entropy and (ii) margin between the top-2
class probabilities from the text-first head. A fallback rule can upgrade NO$\rightarrow$FULL
when entropy or margin exceeds a tunable threshold. Thresholds are selected on a held-out
validation split with a fixed compute budget.

\paragraph{Random-matched and oracle baselines.}
Random-matched baselines sample actions from global or bucketed ratios to match a target
distribution (e.g., policy or oracle). The oracle chooses the cheapest action that yields
the correct answer on a held-out subset, and is used only for analysis.

\paragraph{Compositional baselines (policy + pruning/merge).}
We also evaluate compositional baselines that first select an action with the policy, then
apply pruning or token-merge only to the chosen vision branch (coarse/full). We compute
action-specific token counts and expected cost under the chosen pruning/merge ratio, while
keeping prompts and decoding identical to the main policy.

\paragraph{Profiling methodology.}
Latency and memory are measured on a single GPU with batch size 1. We discard a warm-up
prefix, synchronize CUDA, and report median (P50) and tail (P90) latencies across samples.
Token throughput is reported as decoded tokens per second under the same settings.

\paragraph{Hardware and software.}
Experiments are run on 8$\times$V100 (32\,GB) GPUs.
Software: Python 3.10, PyTorch 2.1.2+cu118, Transformers 4.44.2.
CUDA 11.8 and NCCL are used for multi-GPU training.

\section{Additional Experimental Results}
\label{app:extra}
\paragraph{Resolution and token-merge sweeps.}
We report the full resolution-scaling sweep (224/336/448 long-side) and token-merge sweep
(merge ratios 1.00/0.75/0.50/0.25) on MMBench-dev, for both exact and fuzzy matching.
We also include compositional baselines (policy + pruning/merge).
See \cref{tab:appendix_resolution,tab:appendix_merge,tab:appendix_compose}.

\begin{table}[t]
  \caption{Resolution scaling sweep on MMBench-dev (exact/fuzzy).}
  \label{tab:appendix_resolution}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{cccccc}
    \toprule
    Setting & Split & Acc$\uparrow$ & Cost$\downarrow$ & P50 ms$\downarrow$ & P90 ms$\downarrow$ \\
    \midrule
    224 & Exact & 0.685 & 3.573 & 529 & 2222 \\
    336 & Exact & 0.690 & 3.673 & 882 & 2428 \\
    448 & Exact & 0.705 & 4.743 & 1477 & 2366 \\
    224 & Fuzzy & 0.685 & 3.573 & 659 & 2259 \\
    336 & Fuzzy & 0.690 & 3.673 & 475 & 2360 \\
    448 & Fuzzy & 0.705 & 4.743 & 1355 & 2334 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}[t]
  \caption{Token-merge sweep on MMBench-dev (exact/fuzzy).}
  \label{tab:appendix_merge}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{cccccc}
    \toprule
    Merge ratio & Split & Acc$\uparrow$ & Cost$\downarrow$ & P50 ms$\downarrow$ & P90 ms$\downarrow$ \\
    \midrule
    1.00 & Exact & 0.705 & 4.743 & 255 & 1794 \\
    0.75 & Exact & 0.705 & 3.550 & 256 & 1885 \\
    0.50 & Exact & 0.705 & 2.371 & 249 & 555 \\
    0.25 & Exact & 0.705 & 1.193 & 319 & 2256 \\
    1.00 & Fuzzy & 0.705 & 4.743 & 237 & 283 \\
    0.75 & Fuzzy & 0.705 & 3.550 & 244 & 322 \\
    0.50 & Fuzzy & 0.705 & 2.371 & 272 & 1908 \\
    0.25 & Fuzzy & 0.705 & 1.193 & 260 & 1516 \\
    \bottomrule
  \end{tabular}
  }
  \end{table}

\begin{table}[t]
  \caption{Compositional baselines on MMBench-dev (policy + pruning/merge).}
  \label{tab:appendix_compose}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.34\columnwidth}cccc}
    \toprule
    Method & Split & Acc$\uparrow$ & Cost$\downarrow$ & P50 ms$\downarrow$ \\
    \midrule
    Policy + Pruning & Exact & 0.752 & 2.86 & 270 \\
    Policy + Pruning & Fuzzy & 0.752 & 2.86 & 247 \\
    Policy + Merge & Exact & 0.752 & 2.86 & 247 \\
    Policy + Merge & Fuzzy & 0.752 & 2.86 & 247 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}[t]
  \caption{MMBench-dev fuzzy results for fixed and random baselines.}
  \label{tab:appendix_fuzzy}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.32\columnwidth}ccc}
    \toprule
    Method & Acc$_{\text{Fuzzy}}\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    Always-Full & 0.754 & 5.77 & 0/0/100 \\
    Always-Coarse & 0.740 & 3.54 & 0/100/0 \\
    No-Vision & 0.570 & 0.00 & 100/0/0 \\
    Random Matched (global) & 0.678 & 3.18 & 32/33/35 \\
    Random Matched (bucketed) & 0.674 & 3.13 & 33/33/34 \\
    \method (ours) & 0.752 & 5.72 & 1/0/99 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\begin{table}[t]
  \caption{Policy Pareto sweep on MMBench-dev (exact).}
  \label{tab:appendix_pareto}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{cccc}
    \toprule
    $\lambda_{\text{cost}}$ & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    0.00 & 0.752 & 5.72 & 1/0/99 \\
    0.01 & 0.722 & 4.90 & 13/0/87 \\
    0.02 & 0.646 & 1.86 & 60/0/40 \\
    0.05 & 0.572 & 0.01 & 100/0/0 \\
    0.10 & 0.570 & 0.00 & 100/0/0 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
\paragraph{Extended Pareto curves.}
We include per-dataset Pareto curves and action-ratio plots for all baselines
(Always-Full/Coarse/No-Vision, Threshold (entropy/margin), Random Matched (global/bucketed),
Resolution Scaling, Token Pruning, Token Merge, Multi-Granularity, Fallback, Oracle).

\paragraph{Additional ablations.}
We report temperature and prompt-template sensitivity, alternative uncertainty
signals (entropy vs.\ margin), and pooling choices for the policy input.

\paragraph{Profiling breakdowns.}
We provide additional latency/throughput tables across batch sizes and
hardware configurations, including memory scaling with input resolution.

\paragraph{Qualitative analysis.}
We show representative success/failure cases with predicted action,
model output, and error category.

\section{Reproducibility Checklist}
\label{app:repro}
\begin{itemize}
  \item Exact command lines for training and evaluation (see \texttt{docs/EXPERIMENTS.md}).
  \item Random seed 42 for all runs; multi-seed sweeps report mean and std.
  \item Full hardware/software stack (GPU type, CUDA, PyTorch, Transformers).
  \item Dataset preprocessing scripts and generated JSONL file hashes.
  \item An anonymized code/data link for review (supplementary material).
\end{itemize}

\end{document}
