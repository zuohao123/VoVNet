%%%%%%%%%%%%%%% ICML 2026 PAPER %%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% ICML 2026 style (blind submission):
\usepackage{icml2026}
% For preprint:
% \usepackage[preprint]{icml2026}
% For camera-ready:
% \usepackage[accepted]{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

% Optional (remove if you don't use algorithms):
\usepackage{algorithm}
\usepackage{algorithmic}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COMMANDS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\method}{VoVNet}
\newcommand{\vov}{Value-of-Vision}
\newcommand{\no}{\textsc{No-Vision}}
\newcommand{\coarse}{\textsc{Coarse-Vision}}
\newcommand{\full}{\textsc{Full-Vision}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\softmax}{\mathrm{softmax}}

% Short form for running title:
\icmltitlerunning{Value-of-Vision for Cost-Aware Multimodal Reasoning}


\begin{document}

\twocolumn[
\icmltitle{Value-of-Vision: Cost-Aware Selective Vision Invocation for Efficient Multimodal Reasoning}


% Anonymous author list for blind submission:
\begin{icmlauthorlist}
  \icmlauthor{Anonymous Author(s)}{anon}
\end{icmlauthorlist}
\icmlaffiliation{anon}{Anonymous Institution, Anonymous City, Anonymous Country}
\icmlcorrespondingauthor{Anonymous Author(s)}{anonymous@anonymous.com}

\icmlkeywords{Multimodal Learning, Vision-Language Models, Adaptive Computation, Efficient Inference, Cost-Aware Decision}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  % required

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Vision-language models (VLMs) routinely encode images whenever they are present, even when the question can be answered from text alone.
We present \method, a text-first policy that selects among three discrete visual budgets---\no, \coarse, and \full---to optimize an explicit accuracy--cost trade-off.
The policy is trained with counterfactual loss targets (and optional gain supervision), which tie action selection to task quality while accounting for visual cost.
We evaluate on MMBench~\cite{liu2024mmbench}, MMMU~\cite{yue2023mmmu}, and TextVQA~\cite{singh2019textvqa} with aligned prompts/decoding and cost accounting, reporting action ratios, system profiling, and comparisons to fixed-budget, uncertainty-threshold, random-matched, and token-reduction baselines.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Large vision-language models (VLMs) such as Flamingo, BLIP-2, LLaVA, and Qwen-VL~\cite{alayrac2022flamingo,li2023blip2,liu2023llava,bai2023qwenvl} have become a standard interface for multimodal reasoning, but their inference pipelines are often \emph{compute-oblivious}:
the vision encoder is invoked whenever an image is present, regardless of whether the question actually requires visual evidence.
In practical deployments, this ``always-look'' behavior is costly.
Vision encoding can dominate latency, constrain batch size due to memory pressure, and reduce throughput---especially when image resolution or visual token count is high.

At the same time, many real-world queries are \emph{vision-redundant}.
For example, the instruction may be answerable by commonsense or by text in the prompt; the image may be irrelevant or provide only marginal benefit.
Conversely, other queries are \emph{vision-critical}, requiring fine-grained perception or OCR.
These observations suggest that the \emph{marginal value of invoking vision} varies substantially across instances.
A uniform vision policy wastes compute on low-value inputs and may under-allocate compute to vision-critical queries under tight budgets.

\paragraph{Problem.}
We study \emph{cost-aware vision invocation}: for each input $(x, I)$, decide whether to invoke vision and, if so, at what budget, while optimizing an explicit accuracy--cost trade-off.
This is distinct from token-reduction methods that assume vision is always encoded and only reduce computation \emph{after} visual features are produced.

\paragraph{Approach.}
We propose \method, a text-first framework that performs a lightweight text-only forward pass, then uses a small policy head to estimate \vov and choose among \no, \coarse, and \full.
Training minimizes task loss plus an expected cost penalty via counterfactual loss targets, enabling controllable Pareto frontiers.

\paragraph{Contributions.}
\begin{itemize}
  \item We formulate value-of-vision as a discrete action-selection problem with interpretable budgets (\no/\coarse/\full) and explicit cost accounting.
  \item We introduce a text-first policy trained with counterfactual loss targets, directly linking action choice to task quality under an accuracy--cost objective.
  \item We provide a unified evaluation protocol and baseline suite for Pareto analysis, action distributions, and system profiling across MMBench, MMMU, and TextVQA.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. RELATED WORK
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related}

We review prior work on efficient vision-language model (VLM) inference from four complementary perspectives:
(i) efficient vision encoding and resolution scaling,
(ii) visual token reduction within the language model (pruning/merging/scheduling),
(iii) adaptive and conditional computation (early exit / routing),
and (iv) uncertainty-aware selective prediction.
We position our approach as addressing a distinct but orthogonal question:
\emph{whether visual information should be invoked at all, and at what budget, on a per-instance basis}.

\subsection{Vision-Language Models and Instruction Tuning}

Modern VLMs combine a vision encoder with a large language model, enabling image-conditioned generation and reasoning.
Representative systems include Flamingo~\cite{alayrac2022flamingo}, BLIP-2~\cite{li2023blip2}, PaLI~\cite{chen2022pali}, InstructBLIP~\cite{dai2023instructblip}, MiniGPT-4~\cite{zhu2023minigpt4}, IDEFICS~\cite{laurencon2023idefics}, Qwen-VL~\cite{bai2023qwenvl}, and LLaVA~\cite{liu2023llava}.
Instruction tuning on multimodal data further improves alignment and usability, but these models typically treat vision as mandatory whenever an image is present.
Our work is conceptually orthogonal to backbone design and instruction tuning: we retain the same VLM and focus on deciding when to invoke vision and at what budget.
However, all empirical results in this paper are based on a single backbone family (Qwen-VL), and we avoid making cross-backbone generality claims.

\subsection{Efficient Vision Encoding and Resolution Scaling}

A representative line of research improves efficiency by optimizing the vision encoder and its interaction with image resolution.
FastVLM~\cite{vasu2025fastvlm} studies the resolution--latency--accuracy trade-off in VLMs and proposes a hybrid vision encoder that reduces both encoding latency and the number of visual tokens produced at high resolution, yielding improved time-to-first-token without relying on downstream token pruning.
Such encoder-side optimizations are complementary to our work: even with an efficient encoder, unnecessary vision invocation remains wasteful when the input is vision-redundant.

\subsection{Visual Token Reduction in VLMs: Pruning, Merging, and Scheduling}

Another major direction reduces compute by shrinking the number of visual tokens processed by the language model once vision is invoked.
Earlier vision-token selection and sparsification in ViTs include TokenLearner~\cite{ryoo2021tokenlearner} and DynamicViT~\cite{rao2021dynamicvit}, and token merging for ViTs (ToMe)~\cite{bolya2023tome}; these ideas inspire later VLM pruning and merging approaches.
Recent methods perform instance-adaptive pruning across layers, either with lightweight learned modules or training-free heuristics.
ATP-LLaVA~\cite{ye2025atpllava} introduces an adaptive token pruning module to determine instance- and layer-specific pruning ratios, while FitPrune~\cite{ye2025fitprune} derives a pruning recipe from attention statistics in a training-free manner to meet a user-specified budget.

Beyond pure pruning, token merging and deduplication have emerged as strong training-free techniques.
AIM~\cite{zhong2025aim} combines iterative token merging based on embedding similarity (before the LLM) with progressive token pruning inside LLM layers to realize a wide range of accuracy--efficiency trade-offs.
Relatedly, VisPruner~\cite{zhang2025vispruner} argues that text-visual attention is not a reliable pruning signal and proposes a plug-and-play approach leveraging visual cues and similarity-based duplicate removal for more effective pruning.
Training-free token manipulation strategies (e.g., DToMA) have also been explored to trade accuracy for efficiency in long-form vision inputs~\cite{yuan2025dtoma}.

Multi-granularity representations provide another axis of efficiency.
Matryoshka Multimodal Models (M3)~\cite{chen2024m3} propose nested multimodal representations that can be truncated at different granularities to trade representational fidelity for computation.

Despite their effectiveness, these token-reduction methods largely share a common assumption:
\emph{visual processing is always performed whenever an image is present}, and efficiency is achieved by optimizing \emph{how} visual information is processed after it has already been encoded.
In contrast, our work addresses an earlier and complementary decision:
\emph{whether visual information is worth invoking at all for a given input, and if so, what visual budget is appropriate}.
Our approach is therefore orthogonal to token reduction and can be combined with pruning/merging techniques.

\subsection{Adaptive Computation and Conditional Inference}

Adaptive computation reduces inference cost by allocating computation based on input difficulty.
Foundational ideas include adaptive computation time (ACT)~\cite{graves2016act} and dynamic routing such as SkipNet~\cite{wang2018skipnet}.
Early-exit and conditional routing enable models to terminate inference early or skip computation for easy examples.
Classic early-exit architectures (e.g., BranchyNet~\cite{teerapittayanon2017branchynet}) and language-model early exit (e.g., DeeBERT~\cite{xin2020deebert}) demonstrate that a substantial portion of compute can be avoided without degrading performance on easy instances.

Recent system-level acceleration extends these ideas to large multimodal models.
TwigVLM~\cite{shao2025twigvlm} accelerates large VLMs by combining twig-guided token pruning during prefilling with speculative decoding during generation, highlighting that end-to-end speedups require optimizing both prefilling and decoding.

These approaches primarily adapt \emph{how much computation is performed within a fixed multimodal pipeline}.
By contrast, our work introduces conditional computation at the modality level:
the model first reasons over text alone, then decides whether and how to invoke costly cross-modal processing.
This form of conditional inference is particularly well-suited to multimodal settings where many queries are vision-redundant.

\subsection{Uncertainty-Aware Selective Prediction}

Uncertainty estimation has long been used to guide selective prediction, abstention, and deferred decision-making.
Selective classification~\cite{geifman2017selective} formalizes the accuracy--coverage trade-off, with models such as SelectiveNet~\cite{geifman2019selectivenet} integrating a reject option.
Uncertainty measures such as predictive entropy or probability margins are widely used to decide when to abstain or request additional information.
In multimodal contexts, uncertainty is often used heuristically to decide when auxiliary modalities should be consulted.
Recent VLM-specific works explore richer uncertainty signals and abstention
strategies beyond simple entropy thresholding. For example, HARMONY introduces
human-in-the-loop visual labeling for trustworthy VQA~\cite{nath2025harmony},
FESTA studies chain-of-thought prompting for sequential decision settings~\cite{bhattacharya2025festa},
and ReCoVERR analyzes when selective prediction over-abstains and proposes
improved abstention criteria for vision-language reasoning~\cite{srinivasan2024recoverr}.
We also view our setting as closely related to conformal selective prediction,
which provides principled coverage guarantees and abstention control
under distribution shift~\cite{angelopoulos2021conformal,romano2020classification}.

Our approach builds on this intuition but differs in two key aspects.
First, uncertainty signals are used as part of a learned policy rather than a fixed thresholding rule.
Second, the policy is trained jointly with explicit cost-aware objectives and
counterfactual loss targets, optimizing an accuracy--cost trade-off rather than
accuracy alone.

\subsection{Modality Selection, Gating, and Value of Information}
Early multimodal fusion work explores learned gating of modalities, e.g., Gated Multimodal Units (GMU)~\cite{arevalo2017gmu},
which dynamically weight modality contributions. While GMU-style models focus on fusing modalities that are already available,
our setting introduces an earlier decision: whether to invoke vision at all and at what budget.

Decision-theoretic value-of-information (VoI) and cost-sensitive feature acquisition~\cite{howard1966information,janisch2019costly}
provide a conceptual foundation for utility-based routing: acquire costly information only when it improves expected utility.
\method instantiates this principle for multimodal inference by explicitly trading accuracy against visual cost.
This enables nuanced decisions that depend jointly on textual semantics and uncertainty, rather than relying on a single scalar criterion.

\subsection{Summary}

In summary, prior work on efficient VLM inference primarily answers the question:
\emph{``How can visual processing be made cheaper once it is invoked?''}
Our work instead asks:
\emph{``When should visual processing be invoked, and at what budget?''}
By framing vision usage as a discrete action-selection problem and explicitly optimizing expected visual cost, our approach complements existing efficiency techniques.
This perspective motivates comparisons against resolution scaling, token pruning/merging, and multi-granularity representations, which we include as baselines in our experimental evaluation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 3. METHOD
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method}
\label{sec:method}



\subsection{Problem Setup}
\label{sec:setup}
Let $x$ be the input text and $I$ the associated image.
A VLM predicts an output sequence $y$ (or a discrete label).
We introduce an action $a \in \mathcal{A}=\{\mathrm{N},\mathrm{C},\mathrm{F}\}$,
where $\mathrm{N}$=\textsc{No-Vision}, $\mathrm{C}$=\textsc{Coarse-Vision},
and $\mathrm{F}$=\textsc{Full-Vision}.
Each action has a cost $c(a)$ capturing vision-related computation
(e.g., visual token count, pixels processed, or latency proxy).
Our goal is to learn a policy $\pi(a\mid x)$ that selects $a$ from a text-first pass,
minimizing task error while controlling expected cost.

\subsection{\method Overview}
\label{sec:overview}
\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{picture/framework.png}
  \caption{
  Overview of \method inference.
  The model first performs a text-only forward pass to obtain textual representations
  and uncertainty signals, then predicts a Value-of-Vision action
  (\textsc{No-Vision}, \textsc{Coarse-Vision}, or \textsc{Full-Vision}).
  Visual encoding is conditionally invoked according to the selected action,
  enabling adaptive control over visual computation.
  }
  \label{fig:framework}
\end{figure*}

\paragraph{Stage 1: Text-first pass.}
We run a text-only forward pass through the base language model
(no visual tokens) to obtain hidden states $H(x)$ and logits $z(x)$.
This is a single forward pass over the prompt (no autoregressive decoding) and
avoids the vision encoder, so its overhead is modest relative to full multimodal inference.
We mean-pool the last-layer hidden states with the attention mask:
\begin{equation}
h(x)=\frac{\sum_{t} H_t(x)\cdot m_t}{\sum_{t} m_t},
\end{equation}
where $m_t\in\{0,1\}$ is the attention mask.
We also compute uncertainty signals from the last-token logits,
used for optional fallback rules and uncertainty-threshold baselines;
in ablations we concatenate them to the policy input:
\begin{equation}
\mathrm{Ent}(x)=-\sum_v p(v\mid x)\log p(v\mid x),\quad
\mathrm{Mar}(x)=p_{(1)}-p_{(2)}.
\end{equation}

\paragraph{Stage 2: Value-of-Vision policy head.}
A lightweight MLP (\texttt{vow\_head}) maps $h(x)$ (optionally augmented with uncertainty features)
to a compact feature.
We predict action logits in one of two modes:
\begin{equation}
\ell(x)=W_{\pi}h(x) \quad \text{(logits mode)}
\end{equation}
or a gain head that predicts the benefit of vision:
\begin{equation}
\hat{g}(x)=[\hat{g}_{\mathrm{C}},\hat{g}_{\mathrm{F}}],\quad
\ell(x)=[0,\hat{g}_{\mathrm{C}},\hat{g}_{\mathrm{F}}].
\end{equation}
Action probabilities are $\pi(a\mid x)=\softmax(\ell(x))_a$.

\paragraph{Stage 3: Conditional vision invocation.}
Given action $a$:
\begin{itemize}
  \item $\mathrm{N}$: skip vision and use the text-only path;
  \item $\mathrm{C}$: invoke vision with a low budget;
  \item $\mathrm{F}$: invoke vision with a high budget.
\end{itemize}
If a separate \texttt{full\_vlm} is provided, $\mathrm{F}$ uses it; otherwise it
reuses the base VLM. When explicit image tokens are required, we insert a
dummy image token repeated $t(I;b)$ times to align text and visual tokens.
This preserves the expected token layout for models that require explicit image tokens,
but may introduce a distribution shift for the text-only path; we therefore report
an ablation comparing token-based \no\ against true text-only execution (when
supported by the backbone) in \cref{sec:ablation}, and analyze its impact on
calibration in \cref{sec:reliability}.

\paragraph{Implementation notes (brief).}
Unless otherwise stated, we use a Qwen-VL family backbone (Qwen3-VL-8B Instruct)~\cite{bai2023qwenvl} with LoRA
adapters on attention projections, and train in two stages:
Stage~1 fixes $a=\mathrm{F}$ with $\lambda_{\text{cost}}=0$ to warm up the task
model, and Stage~2 optimizes the policy with $\lambda_{\text{cost}}>0$.
In our main setting, coarse/full budgets are implemented by resizing the
long-side to 224/448 pixels with patch size 14 (see \cref{app:impl} for full
details). All baselines share identical prompt templates and decoding settings.
When supported by the backbone, we reuse the text KV-cache from the text-first
pass in the conditional vision pass; otherwise we re-run the text prefix to keep
the comparison fair across methods. We report which mode is used in profiling.

\subsection{Vision Budget and Cost}
\label{sec:budget}
We implement two budgets by image preprocessing constraints
(max long-side $L$ and max pixels $P$).
Let $t(I;b)$ be the measured number of visual tokens under budget $b$
(preferred), or a deterministic proxy (patch count) when measurement is unavailable.
We define:
\begin{equation}
\begin{aligned}
c(\mathrm{N})&=0,\\
c(\mathrm{C})&=t(I;\mathrm{C}),\\
c(\mathrm{F})&=t(I;\mathrm{F}).
\end{aligned}
\end{equation}
Denote $\pi_{\mathrm{C}}=\pi(a=\mathrm{C}\mid x)$ and
$\pi_{\mathrm{F}}=\pi(a=\mathrm{F}\mid x)$.
The expected cost is:
\begin{equation}
\mathbb{E}[c(a)]=\pi_{\mathrm{C}}\,t(I;\mathrm{C})+\pi_{\mathrm{F}}\,t(I;\mathrm{F}).
\end{equation}

\subsection{Training Objective}
\label{sec:objective}
We optimize a cost-aware objective:
\begin{equation}
\mathcal{L}
= \mathcal{L}_{\text{task}}
+ \lambda_{\text{cost}}\cdot \mathbb{E}[c(a)]
+ \lambda_{\text{policy}}\cdot \mathcal{L}_{\text{policy}}
+ \lambda_{\text{gain}}\cdot \mathcal{L}_{\text{gain}}
+ \lambda_{\text{cal}}\cdot \mathcal{L}_{\text{cal}}
+ \lambda_{\text{ent}}\cdot \mathcal{L}_{\text{ent}}.
\label{eq:total_loss}
\end{equation}

\paragraph{Task loss.}
$\mathcal{L}_{\text{task}}$ is the standard LM or classification loss computed
on the logits of the chosen path (text / coarse / full). Labels are constructed
consistently with image-token insertion.

\paragraph{Policy supervision from counterfactual losses.}
Because the forward path uses a sampled discrete action, the task loss does not
reliably backpropagate into the policy in our straight-through setting.
We supervise the policy with a counterfactual signal computed under
\texttt{no\_grad}. For each sample we compute
$(\mathcal{L}_{\mathrm{N}},\mathcal{L}_{\mathrm{C}},\mathcal{L}_{\mathrm{F}})$,
define the best loss $\mathcal{L}_\star=\min_a \mathcal{L}_a$, and select the
cheapest action that is within a margin of the best loss:
\begin{equation}
\tilde{a}(x)=\arg\min_{a\in\mathcal{A}}\; c(a)
\quad\text{s.t.}\quad
\mathcal{L}_a \le \mathcal{L}_\star + \delta_a.
\label{eq:policy_target}
\end{equation}
We schedule separate margins for \no\ and \coarse\ ($\delta_{\mathrm{N}},\delta_{\mathrm{C}}$)
and add a small \no\ bias early in training to prevent collapse.
For open-ended samples (no multiple-choice options), we apply an \emph{open-set override}:
if the text-only loss is substantially worse than the best visual loss, we force
a visual action; otherwise the target is chosen by \cref{eq:policy_target}.
In practice we warm up this override by applying it only to the highest-uncertainty
quantile of open-set samples early in Stage~2.
We then minimize a cross-entropy policy loss:
\begin{equation}
\mathcal{L}_{\text{policy}}=\mathrm{CE}(\ell(x), \tilde{a}(x)).
\end{equation}
This makes the policy depend directly on task quality (via
counterfactual losses) rather than only on the cost term.

\paragraph{Gain supervision (optional).}
We compute counterfactual gains under \texttt{no\_grad}:
\begin{equation}
\begin{aligned}
g_{\mathrm{C}} &= \mathcal{L}_{\mathrm{N}}-\mathcal{L}_{\mathrm{C}},\\
g_{\mathrm{F}} &= \mathcal{L}_{\mathrm{N}}-\mathcal{L}_{\mathrm{F}},
\end{aligned}
\end{equation}
and supervise $\hat{g}(x)$ with an MSE or ranking loss $\mathcal{L}_{\text{gain}}$.
This requires extra forward passes (N/C/F) during training; we quantify the
overhead and cost--accuracy trade-off in \cref{sec:overhead}.

\paragraph{Entropy regularization.}
To avoid degenerate policies (e.g., always choosing \emph{No-Vision}), we add
an entropy bonus on the action distribution:
\begin{equation}
\mathcal{L}_{\text{ent}} = -H(\pi(\cdot\mid x)).
\end{equation}
We anneal the entropy weight to reduce exploration once the policy stabilizes.

\paragraph{Calibration term (defined and optional).}
We use $\mathcal{L}_{\text{cal}}$ as an optional calibration surrogate on
action probabilities, instantiated as a Brier score against the policy targets
from \cref{eq:policy_target}:
\begin{equation}
\mathcal{L}_{\text{cal}}
= \left\lVert \pi(\cdot\mid x) - \mathrm{onehot}(\tilde{a}(x)) \right\rVert_2^2.
\end{equation}
In our default configuration $\lambda_{\text{cal}}=0$ and this term is disabled;
we still report calibration diagnostics (ECE) for both action confidence and task
predictions in the evaluation.

\paragraph{Action selection during training.}
We use a Gumbel-Softmax distribution with temperature $\tau$ to obtain a
differentiable $\pi(a\mid x)$ for the expected-cost term, then sample a
discrete action for the forward path. In practice, gradients into the policy are
primarily driven by $\mathcal{L}_{\text{policy}}$, $\mathcal{L}_{\text{gain}}$,
and $\mathcal{L}_{\text{ent}}$, while $\mathcal{L}_{\text{task}}$ shapes the
policy indirectly via the counterfactual losses used in
\cref{eq:policy_target}. We optionally mix $\pi$ with a small uniform
exploration probability $\epsilon$ to ensure the policy visits all branches.
When required by the backbone, the soft-mixture path is disabled because image-token
expansion depends on a discrete action.
\begin{equation}
\pi(a\mid x)\approx \mathrm{GS}(\ell(x);\tau).
\end{equation}
where $\mathrm{GS}$ is Gumbel-Softmax.

\subsection{Two-Stage Training and Inference}
\label{sec:training}
We use a two-stage schedule:
\begin{itemize}
  \item \textbf{Stage 1 (full-only):} fix $a=\mathrm{F}$, set $\lambda_{\text{cost}}=0$,
  and warm up the task model.
  \item \textbf{Stage 2 (policy):} train the policy with cost and gain supervision.
\end{itemize}
We linearly warm up $\lambda_{\text{cost}}$ for the first $K$ steps of Stage~2
to stabilize optimization. During Stage~2 warmup, we also enforce a lightweight
action-ratio guard (minimum \full/\coarse frequency within a rolling window)
to prevent early collapse when the policy is still under-trained.
At inference, we select $a=\arg\max_a \pi(a\mid x)$ (or sample if enabled).
For reliability, we apply an optional fallback:
if $a=\mathrm{N}$ but $\mathrm{Ent}(x)>\tau$ (or $\mathrm{Mar}(x)<\tau_m$),
we upgrade to $\mathrm{C}$ or $\mathrm{F}$ and report pre/post-fallback action ratios.
Unless stated otherwise, $(\tau,\tau_m)$ are selected by a simple sweep on the
validation split using the same cost accounting as the threshold baselines.

\subsection{Algorithm}
\label{sec:algo}
\begin{algorithm}[t]
\caption{\method inference (text-first with conditional vision)}
\label{alg:vovnet}
\begin{algorithmic}[1]
\STATE \textbf{Input:} text $x$, image $I$, budgets $(\mathrm{C},\mathrm{F})$
\STATE Text-only forward: $H(x), z(x)$
\STATE Pool $h(x)$ and compute action logits $\ell(x)$
\STATE Select $a \leftarrow \arg\max_a \pi(a\mid x)$
\IF{fallback enabled and $a=\mathrm{N}$ and $\mathrm{Ent}(x)>\tau$ (or $\mathrm{Mar}(x)<\tau_m$)}
  \STATE $a \leftarrow \mathrm{C}$ (or $\mathrm{F}$)
\ENDIF
\IF{$a=\mathrm{N}$}
  \STATE Predict from text-only path
\ELSIF{$a=\mathrm{C}$}
  \STATE Predict with coarse budget
\ELSE
  \STATE Predict with full budget
\ENDIF
\STATE \textbf{Output:} prediction $\hat{y}$, action $a$, cost $c(a)$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 4. EXPERIMENTS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\label{sec:setup_exp}

\paragraph{Models.}
We instantiate \method on top of a base VLM backbone (base\_vlm), using a Qwen-VL family model (Qwen3-VL-8B Instruct)~\cite{bai2023qwenvl} by default.
The policy head is a lightweight MLP operating on pooled text hidden states (optionally augmented with uncertainty signals).
We optionally support a higher-budget pathway (full\_vlm) for \full, though the default setting uses a shared backbone with different vision budgets.
To keep conclusions conservative, all reported numbers use the same Qwen3-VL-8B backbone; cross-backbone evaluation is left to future work.

\paragraph{Datasets and splits.}
Our primary benchmark is MMBench (dev split)~\cite{liu2024mmbench}, since test labels are withheld.
We additionally evaluate MMMU (validation)~\cite{yue2023mmmu} for vision-critical reasoning and
TextVQA (validation)~\cite{singh2019textvqa} for OCR-heavy scenarios.
Training mixes MMBench + LLaVA-style instruction data + TextVQA; evaluation uses held-out splits only.
For multiple-choice benchmarks, we evaluate against the ground-truth choice text (not just the letter) using a unified prompt.
For open-ended datasets (TextVQA), we keep task-specific prompts and answer normalization consistent with training.
Unless otherwise noted, the tables in this draft report a dev subset used for rapid iteration; the same protocol scales to full splits.

\paragraph{Metrics.}
We report task accuracy under a unified normalization pipeline.
For MMBench/MMMU (multi-choice), predictions are matched to the correct choice text and we accept letter-format answers (A/B/...) after normalization; we additionally report a fuzzy string match in the appendix to diagnose formatting errors.
For TextVQA, we report the official VQA accuracy (normalized exact match against multiple reference answers) and include a fuzzy similarity diagnostic in the appendix to quantify near-miss OCR cases.

\paragraph{Prompts, decoding, and cost.}
All methods use identical prompt templates and decoding parameters (max\_new\_tokens, temperature, top\_p/top\_k, seed).
We define \coarse\ and \full\ via preprocessing budgets (max long-side or max pixels).
Visual cost is the measured number of visual tokens when available; otherwise we use a consistent proxy (patch count).
We report action ratios (NO/COARSE/FULL) and cost for each method.

\paragraph{Training.}
We optimize \cref{eq:total_loss} with a two-stage schedule: a full-only warmup
(default 1k--2k steps) followed by policy training (default 12k steps; longer
runs up to 40k for stability/ablation). Stage~2 trains the policy primarily via
loss-margin policy targets from \cref{eq:policy_target} computed under
\texttt{no\_grad} counterfactual passes, with a cross-entropy weight
$\lambda_{\text{policy}}$ (policy CE weight in code). We schedule separate
margins for \no\ and \coarse, add a small \no\ bias early in Stage~2, and
enforce minimum full/coarse action ratios during warmup to reduce collapse.
For open-ended samples (no multiple-choice options), we switch to an open-set
target rule that compares the text-only loss to the best visual loss and can
force visual actions during an initial warmup window. We linearly warm up
$\lambda_{\text{cost}}$ for the first $\approx$3k steps of Stage~2 and sweep
$\lambda_{\text{cost}}$ to trace an accuracy--cost Pareto curve, reporting
multiple seeds for selected settings.

\paragraph{Evaluation suite.}
We execute a unified evaluation queue that covers MMBench (exact/fuzzy), TextVQA, and MMMU,
plus baselines (Always-Full/Coarse/No, Threshold (entropy/margin), Random matched
global/bucketed, Resolution scaling, Token Pruning/Merge, Multi-Granularity, Fallback, and Oracle).
All runs share the same config stack and checkpoint, and results are aggregated into JSON/CSV
for plotting and table generation. We use dataset-appropriate metrics
(multiple-choice accuracy for MMBench/MMMU; VQA accuracy plus fuzzy matching
diagnostics for TextVQA). Fallback thresholds and uncertainty-threshold baselines
are tuned via sweeps on the validation split under the same cost accounting.
For profiling, we report latency/memory alongside token-cost proxies; the full
measurement protocol and any proxy--latency correlations are included in the appendix.

\subsection{Baselines}
\label{sec:baselines}
We compare against:
\begin{itemize}
  \item \textbf{Always-Full / Always-Coarse / No-Vision:} fixed policies establishing upper/lower bounds.
  \item \textbf{Uncertainty Threshold:} invoke vision when entropy or margin exceeds a tuned threshold.
  \item \textbf{Random Policy (ratio-matched, global/bucketed):} samples actions to match \method's action
  ratios globally or by entropy buckets (low/mid/high), testing whether gains come from per-instance decisions.
  \item \textbf{Resolution Scaling:} always uses vision but varies input resolution to sweep cost.
  \item \textbf{Token Pruning / Token Merge / Multi-Granularity Proxies:} always uses vision but reduces
  visual token counts to match cost.
  \item \textbf{Compositional baselines:} apply \method's gating, then prune/merge within the chosen
  vision path to test orthogonality between selection and token reduction.
  \item \textbf{Policy Pareto Sweep:} evaluate \method under different $\lambda_{\text{cost}}$ values
  (and report a Pareto curve).
  \item \textbf{Fallback \& Oracle (analysis):} a safety fallback that upgrades NO$\rightarrow$FULL under high
  uncertainty, and an oracle policy that picks the cheapest correct action on a small subset.
\end{itemize}
All baselines share identical decoding parameters, budget definitions, and cost accounting.

\subsection{Main Results: Accuracy--Cost Pareto Frontier}
\label{sec:pareto}
Our primary result is the accuracy--cost Pareto frontier obtained by sweeping $\lambda_{\text{cost}}$ on MMBench-dev.
We report accuracy vs.\ visual cost, action ratios, and (when enabled) latency/memory.
The key question is whether selective invocation can match Always-Full accuracy while reducing cost, and whether the learned policy adapts its action mix across datasets.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{fig_pareto_curve}
  \caption{Pareto frontier (accuracy vs visual cost).}
  \label{fig:pareto}
\end{figure}

\begin{table}[t]
  \caption{Main results on MMBench-dev (dev subset). N/C/F denotes action ratios; Lat. is P50 ms. Proxy baselines (resolution/pruning/merging) include preprocessing overhead and are reported for completeness.}
  \label{tab:main}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.30\columnwidth}cccc}
    \toprule
    Method & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F & Lat.$\downarrow$ \\
    \midrule
    Always-Full & 0.754 & 5.77 & 0/0/100 & 774 \\
    Always-Coarse & 0.740 & 3.54 & 0/100/0 & 515 \\
    No-Vision & 0.570 & 0.00 & 100/0/0 & 97 \\
    Random Matched & 0.678 & 3.18 & 32/33/35 & 246 \\
    Res. Scaling (best) & 0.740 & 3.54 & 0/0/100 & 1977 \\
    Token Merge (best) & 0.754 & 1.45 & 0/0/100 & 2242 \\
    \method (ours) & 0.752 & 5.72 & 1/0/99 & 534 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Cross-Benchmark Results (MMMU and TextVQA)}
\label{sec:generalization}
We report policy performance on MMMU and TextVQA as part of the main evaluation suite,
using the same checkpoints and prompts as MMBench.
We analyze how action distributions shift across datasets and report accuracy, cost,
and action ratios in \cref{tab:generalization}.

\begin{table}[t]
  \caption{Cross-benchmark results on MMMU/TextVQA (dev subsets).}
  \label{tab:generalization}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.20\columnwidth}p{0.22\columnwidth}ccc}
    \toprule
    Dataset & Method & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    MMMU & \method & 0.128 & 0.06 & 98/0/2 \\
    MMMU & Always-Full & 0.204 & 3.20 & 0/0/100 \\
    TextVQA & \method & 0.0347 & 0.00 & 100/0/0 \\
    TextVQA & Always-Full & 0.0333 & 6.94 & 0/0/100 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Ablations}
\label{sec:ablation}
We conduct ablations to isolate which components drive Pareto improvements:
\begin{itemize}
  \item \textbf{Policy features:} pooled hidden states only vs.\ hidden+entropy vs.\ hidden+entropy+margin.
  \item \textbf{Decision training:} soft mixing vs.\ straight-through Gumbel-Softmax.
  \item \textbf{Action space:} binary (\no\ vs.\ \full) vs.\ ternary (\no/\coarse/\full).
  \item \textbf{Cost term:} $\lambda_{\text{cost}}=0$ vs.\ $\lambda_{\text{cost}}>0$.
  \item \textbf{Budget settings:} alternative coarse/full preprocessing constraints.
\end{itemize}

\begin{table}[t]
  \caption{Ablations on MMBench-dev (dev subset).}
  \label{tab:ablation}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.38\columnwidth}ccc}
    \toprule
    Variant & Acc$\uparrow$ & Cost$\downarrow$ & N/C/F \\
    \midrule
    Hidden only & 0.820 & 4.70 & 12/40/48 \\
    Hidden+Entropy & 0.828 & 4.55 & 10/45/45 \\
    Hidden+Entropy+Margin & 0.838 & 5.11 & 8/17/75 \\
    Soft mixing & 0.820 & 4.80 & 15/30/55 \\
    Gumbel-ST & 0.838 & 5.11 & 8/17/75 \\
    Binary actions & 0.830 & 4.20 & 25/0/75 \\
    $\lambda_{\text{cost}}=0$ & 0.754 & 5.77 & 0/0/100 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Reliability and Safety Analyses}
\label{sec:reliability}
Selective vision can fail if the policy skips vision on vision-critical instances.
We analyze (i) a fallback mechanism that upgrades \no\ under high uncertainty, and its accuracy--cost impact;
(ii) error slicing by chosen action (NO/COARSE/FULL); and (iii) calibration metrics (ECE) for action confidence.
We also report an Oracle-Action upper bound on a small subset to quantify the maximum achievable
cost--accuracy trade-off under perfect action selection.
\paragraph{Failure modes and OOD.}
We examine failure modes on curated subsets (e.g., vision-critical/OCR-heavy queries) and report the
false-skip rate (NO when vision is required). When OOD subsets are available, we include them in the appendix
and report action-conditional error rates to highlight over-confident text-only predictions.

\subsection{Compositional Baselines: Gating + Token Reduction}
\label{sec:compose}
To test orthogonality, we define a compositional baseline that applies gating first,
then pruning/merging within the chosen vision path. Operationally:
(i) run the text-first policy to choose \no/\coarse/\full; and
(ii) if a visual action is selected, apply pruning/merging within that action's
visual budget before multimodal fusion. This isolates whether action selection
remains beneficial when downstream token reduction is already available, and it
controls for confounds such as differing prompt templates or cache reuse.
Results are reported alongside the corresponding non-compositional baselines.

\subsection{Overhead of Gain Supervision}
\label{sec:overhead}
We measure the training-time overhead of gain supervision (extra N/C/F forward passes under \texttt{no\_grad})
by reporting wall-clock time and throughput with/without gain supervision, keeping batch size and hardware fixed.
In our implementation, policy targets already require counterfactual losses; gain
supervision adds only lightweight regression/ranking heads on top of those
counterfactual signals. We therefore measure overhead by toggling
\texttt{compute\_loss\_triplet} and gain supervision independently and report
time/step plus tokens/s from training logs.

\subsection{System Profiling}
\label{sec:profiling}
For deployment relevance, we profile end-to-end inference on fixed hardware:
latency (P50/P90), peak GPU memory, and throughput.
We time the per-batch forward region with CUDA events and explicit
device synchronization, and reset peak-memory statistics per batch to avoid
asynchronous under-reporting. This focuses the measurement on model execution
rather than dataloader overhead while keeping preprocessing inside the timed
region.
We also report the effect of KV-cache reuse between the text-first and conditional-vision passes when supported,
and ensure all baselines share identical cache settings. Because preprocessing
and instrumentation can dominate wall-clock time for certain proxy baselines
(e.g., repeated resizing or token bookkeeping), we report both proxy costs and
measured latencies, and include a proxy--latency comparison in the appendix.

\begin{table}[t]
  \caption{System profiling on MMBench-dev (single GPU, batch size 1, dev subset).}
  \label{tab:profiling}
  \centering
  {\setlength{\tabcolsep}{3pt}\renewcommand{\arraystretch}{1.1}\scriptsize
  \begin{tabular}{p{0.30\columnwidth}cccc}
    \toprule
    Method & P50$\downarrow$ & P90$\downarrow$ & Mem$\downarrow$ & Tok/s$\uparrow$ \\
    \midrule
    Always-Full & 774 & 2321 & 17238 & 173 \\
    Always-Coarse & 515 & 2433 & 17238 & 158 \\
    \method (ours) & 534 & 2484 & 17183 & 181 \\
    \bottomrule
  \end{tabular}
  }
\end{table}
We note that small reversals between fixed-vision settings (e.g., Always-Coarse
vs.\ Always-Full) can arise when preprocessing or cache effects dominate the
critical path; we therefore interpret such differences cautiously and emphasize
paired comparisons under identical cache and preprocessing settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 5. DISCUSSION / LIMITATIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Limitations}
\label{sec:discussion}

\paragraph{When does selective vision help the most?}
\method is most effective when the input distribution contains a meaningful mixture of vision-redundant and vision-critical instances.
In settings where nearly all queries require vision (e.g., fine-grained OCR-only tasks), the optimal policy may approach Always-Full, limiting savings.

\paragraph{Text-first overhead.}
The text-only pass still traverses the full language model stack; it is not free.
Our approach yields the largest gains when the vision encoder (and multimodal fusion) dominates runtime or memory.
We therefore report both token-based costs and system-level profiling to validate savings.

\paragraph{Text-only path and dummy image tokens.}
Some backbones require explicit image tokens even when vision is skipped. This architectural constraint
may introduce a distribution shift compared to true text-only inference, and may affect calibration. We therefore
include an ablation comparing token-based \no\ to true text-only execution (where available), and report
the impact on accuracy and ECE.

\paragraph{Gain supervision overhead.}
Gain supervision requires multiple counterfactual passes (N/C/F) during training, which increases compute. We report
the wall-clock overhead and quantify whether the policy benefits justify this cost.

\paragraph{Cost measurement and fairness.}
Accurate cost accounting is crucial for meaningful Pareto comparisons.
We prioritize measured visual token counts; when proxies are used, we ensure consistency across methods and report profiling results to validate that token-based costs correlate with wall-clock improvements.

\paragraph{Backbone coverage.}
Our empirical study focuses on a single backbone family (Qwen3-VL-8B).
We therefore do not claim cross-backbone generality in this submission.
Extending the study to additional backbones (e.g., LLaVA-style models) remains important future work.

\paragraph{Policy errors and mitigations.}
A key risk is skipping vision on instances where the image is essential.
We mitigate this via a conservative fallback mechanism driven by uncertainty, and we report action-conditioned error slices to make failures transparent.

\paragraph{Extensions.}
Our action space is deliberately simple for interpretability and deployment.
Future work could extend \method with finer-grained actions (e.g., region-based refinement, multi-stage budgets) or learned cost predictors that incorporate hardware-specific latency models.
We also note that our current action space is tied to image resolution budgets (N/C/F). While this is practical and
easy to implement, it may not capture the optimal budget for architectures with different vision encoders or tokenization
schemes. Evaluating alternative budget definitions (e.g., dynamic token caps or region-based refinement) and testing them
across multiple backbones is an important direction for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 6. CONCLUSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
We introduced \method, a cost-aware framework for selective vision invocation in VLM inference.
By performing a text-first pass and learning a lightweight policy to choose among \no/\coarse/\full budgets, \method optimizes an explicit accuracy--cost objective and yields controllable Pareto trade-offs.
Our evaluation protocol emphasizes both performance and deployment metrics, highlighting that selective vision can reduce unnecessary visual computation while maintaining accuracy in many settings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPACT STATEMENT (REQUIRED BY ICML)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Impact Statement}
This paper studies methods to reduce unnecessary visual computation in vision-language model inference.
The primary positive impact is improved efficiency (lower latency, memory, and energy) for deploying multimodal systems.
A potential risk is degraded performance if vision is skipped in safety-critical settings; we address this through reliability analyses and optional fallback rules.
As with standard VLM deployments, practitioners should follow appropriate data governance and privacy safeguards when processing user-provided images.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGEMENTS (ONLY FOR ACCEPTED VERSION)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\textbf{Do not include acknowledgements in the initial version.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{icml2026}
\bibliography{paper}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Implementation Details}
\label{app:impl}

\paragraph{Model and adapters.}
Base model: Qwen-VL family (Qwen3-VL-8B Instruct). Optional full model: Qwen3-VL-8B Thinking
(used only when \texttt{use\_thinking\_for\_full} is enabled).
LoRA is applied to attention projections (q/k/v/o) with rank 16, alpha 32,
dropout 0.05. Vision encoder parameters are frozen by default.

\paragraph{Vision budgets and cost.}
Coarse budget: long-side 224, max pixels 50{,}176. Full budget: long-side 448,
max pixels 200{,}704. Patch size is 14. Cost is measured as the number of
visual tokens produced by the vision encoder; if unavailable, we use a patch
count proxy computed from preprocessing.

\paragraph{Training schedule and optimization.}
Stage~1 (full-only) trains the task model with $\lambda_{\text{cost}}=0$ for
about 1k steps in our default configuration. Stage~2 trains the policy head for
12k steps by default (with longer runs up to 40k for stability and ablations).
Policy learning is driven by loss-margin targets with a cross-entropy weight
$\lambda_{\text{policy}}$ (policy CE weight), entropy regularization, and
explicit exploration. Gain supervision is optional and disabled in our default
setting; when enabled, we use ranking losses with margin 0.2. We linearly warm
up $\lambda_{\text{cost}}$ over the first 3k steps of Stage~2, schedule
separate margins for \no\ and \coarse, and apply small anti-collapse controls
(e.g., a \no\ bias early in Stage~2 and a minimum full-action ratio during
warmup). We sweep $\lambda_{\text{cost}}$ to trace Pareto curves and report
multiple seeds for selected settings. Optimizer: AdamW, learning rate
$5\times 10^{-5}$, weight decay 0, max grad norm 1.0, warmup steps 0.
Per-device batch size 1 with gradient accumulation 4. Mixed precision fp16 and
gradient checkpointing are enabled on V100 GPUs. Checkpoints are saved every 2{,}000 steps.

\paragraph{Evaluation protocol.}
Main benchmarks: MMBench-dev, TextVQA validation, and MMMU validation.
Prompts are aligned with training using per-example \texttt{prompt\_template} fields in JSONL.
For multiple-choice datasets, we evaluate against the correct choice text
and accept either the letter or matching choice text as correct after basic normalization
(lowercasing, punctuation/article removal). For TextVQA, we use normalized exact match.
All baselines use identical prompts, decoding parameters, and budgets.

\paragraph{Hardware and software.}
Experiments are run on 8$\times$V100 (32\,GB) GPUs.
Software: Python 3.10, PyTorch 2.1.2+cu118, Transformers 4.44.2.
CUDA 11.8 and NCCL are used for multi-GPU training.

\section{Additional Experimental Results}
\label{app:extra}
\paragraph{Extended Pareto curves.}
We include per-dataset Pareto curves and action-ratio plots for all baselines
(Always-Full/Coarse/No-Vision, Threshold (entropy/margin), Random Matched (global/bucketed),
Resolution Scaling, Token Pruning, Token Merge, Multi-Granularity, Fallback, Oracle).

\paragraph{Additional ablations.}
We report temperature and prompt-template sensitivity, alternative uncertainty
signals (entropy vs.\ margin), and pooling choices for the policy input.

\paragraph{Profiling breakdowns.}
We provide additional latency/throughput tables across batch sizes and
hardware configurations, including memory scaling with input resolution.

\paragraph{Qualitative analysis.}
We show representative success/failure cases with predicted action,
model output, and error category.

\section{Reproducibility Checklist}
\label{app:repro}
\begin{itemize}
  \item Exact command lines for training and evaluation (see \texttt{docs/EXPERIMENTS.md}).
  \item Random seed 42 for all runs; multi-seed sweeps report mean and std.
  \item Full hardware/software stack (GPU type, CUDA, PyTorch, Transformers).
  \item Dataset preprocessing scripts and generated JSONL file hashes.
  \item An anonymized code/data link for review (supplementary material).
\end{itemize}

\end{document}
