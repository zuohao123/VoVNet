%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Benchmarks / Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{liu2024mmbench,
  title     = {MMBench: Is Your Multi-modal Model an All-around Player?},
  author    = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and Chen, Kai and Lin, Dahua},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year      = {2024},
  doi       = {10.1007/978-3-031-72658-3_13}
}

@article{yue2023mmmu,
  title   = {MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
  author  = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal = {arXiv preprint arXiv:2311.16502},
  year    = {2023},
  doi     = {10.48550/arXiv.2311.16502}
}

@article{singh2019textvqa,
  title   = {Towards VQA Models That Can Read},
  author  = {Singh, Amanpreet and Natarjan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  journal = {arXiv preprint arXiv:1904.08920},
  year    = {2019},
  doi     = {10.48550/arXiv.1904.08920}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Vision-Language Models / Instruction Tuning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{alayrac2022flamingo,
  title     = {Flamingo: a Visual Language Model for Few-Shot Learning},
  author    = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Brock, Andrew and Doersch, Carl and Fan, Angela and Fergus, Rob and Gray, Alexander and Hadsell, Raia and Huang, Po-Sen and Kavukcuoglu, Koray and Li, Michael and Vinyals, Oriol and Wierstra, Daan and Zisserman, Andrew and Simonyan, Karen},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}

@inproceedings{li2023blip2,
  title     = {BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author    = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven C. H.},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2023}
}

@article{chen2022pali,
  title   = {PaLI: A Jointly-Scaled Multilingual Language-Image Model},
  author  = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Lee, Kyunghyun and Wang, Lijun and Yu, Yukun and Tuzel, Oncel and Wu, Jiawei and Goodman, Noah and Li, Yonghui and Chen, Wenliang},
  journal = {arXiv preprint arXiv:2209.06794},
  year    = {2022},
  doi     = {10.48550/arXiv.2209.06794}
}

@article{bai2023qwenvl,
  title   = {Qwen-VL: A Versatile Vision-Language Model},
  author  = {Bai, Jinze and Bai, Shuai and Chu, Yun and Cui, Ziniu and Dang, Kai and Deng, Xiaoyi and Fan, Yang and Fang, Shihan and Gao, Bowen and Guo, Hongjin and Huang, Haotian and Huang, Jiaqi and Li, Bing and Li, Yuhui and Li, Yuze and Liu, Chujie and Liu, Jing and Liu, Kai and Liu, Yang and Liu, Yujie and Ma, Junxing and Ning, Qinyuan and Qiu, Qipeng and Ren, Renjie and Tan, Shifeng and Wang, Jingdong and Wang, Wenyu and Wen, Zhisheng and Wu, Yixin and Xu, Bin and Xu, Dongkuan and Xu, Jun and Yang, Cheng and Yang, Shijun and Zhou, Ziyi and Zhu, Junnan},
  journal = {arXiv preprint arXiv:2308.12966},
  year    = {2023},
  doi     = {10.48550/arXiv.2308.12966}
}

@article{liu2023llava,
  title   = {Visual Instruction Tuning},
  author  = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal = {arXiv preprint arXiv:2304.08485},
  year    = {2023},
  doi     = {10.48550/arXiv.2304.08485}
}

@article{dai2023instructblip,
  title   = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author  = {Dai, Wenliang and others},
  journal = {arXiv preprint arXiv:2305.06500},
  year    = {2023},
  doi     = {10.48550/arXiv.2305.06500}
}

@article{zhu2023minigpt4,
  title   = {MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author  = {Zhu, Deyao and others},
  journal = {arXiv preprint arXiv:2304.10592},
  year    = {2023},
  doi     = {10.48550/arXiv.2304.10592}
}

@article{laurencon2023idefics,
  title   = {IDEFICS: An Open Access Foundation Model for Multimodal Generation},
  author  = {Laurencon, Hugo and others},
  journal = {arXiv preprint arXiv:2306.16527},
  year    = {2023},
  doi     = {10.48550/arXiv.2306.16527}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Efficient Vision Encoding / Resolution Scaling
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{vasu2025fastvlm,
  title     = {FastVLM: Efficient Vision Encoding for Vision-Language Models},
  author    = {Vasu, Pavan Kumar Anasosalu and Faghri, Fartash and Li, Chun-Liang and Koc, Cem and True, Nate and Antony, Albert and Santhanam, Gokul and Gabriel, James and Grasch, Peter and Tuzel, Oncel and Pouransari, Hadi},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Visual Token Pruning / Merging / Manipulation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{ryoo2021tokenlearner,
  title     = {TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?},
  author    = {Ryoo, Michael S. and others},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2021}
}

@inproceedings{rao2021dynamicvit,
  title     = {DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},
  author    = {Rao, Yongming and others},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2021}
}

@inproceedings{bolya2023tome,
  title     = {Token Merging: Your ViT but Faster},
  author    = {Bolya, Daniel and others},
  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},
  year      = {2023}
}

@inproceedings{ye2025atpllava,
  title     = {ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models},
  author    = {Ye, Xubing and Gan, Yukang and Ge, Yixiao and Zhang, Xiao-Ping and Tang, Yansong},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2025}
}

@inproceedings{ye2025fitprune,
  title     = {Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models},
  author    = {Ye, Weihao and Wu, Qiong and Lin, Wenhao and Zhou, Yiyi},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year      = {2025},
  doi       = {10.1609/aaai.v39i21.34366}
}

@inproceedings{zhong2025aim,
  title     = {AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning},
  author    = {Zhong, Yiwu and Liu, Zhuoming and Li, Yin and Wang, Liwei},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}

@inproceedings{zhang2025vispruner,
  title     = {Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs},
  author    = {Zhang, Qizhe and Cheng, Aosong and Lu, Ming and Zhang, Renrui and Zhuo, Zhiyong and Cao, Jiajun and Guo, Shaobo and She, Qi and Zhang, Shanghang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}

@inproceedings{yuan2025dtoma,
  title     = {DToMA: Training-free Dynamic Token Manipulation for Long Video Understanding},
  author    = {Yuan, Bowen and You, Sisi and Bao, Bing-Kun},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  year      = {2025},
  doi       = {10.24963/ijcai.2025/258}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multi-granularity / Nested Representations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{chen2024m3,
  title   = {Matryoshka Multimodal Models},
  author  = {Chen, Zhe and Li, Mingda and Zhang, Yuhui and Chen, Wenhu},
  journal = {arXiv preprint arXiv:2406.04330},
  year    = {2024},
  doi     = {10.48550/arXiv.2406.04330}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Adaptive Computation / Early Exit
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{graves2016act,
  title   = {Adaptive Computation Time for Recurrent Neural Networks},
  author  = {Graves, Alex},
  journal = {arXiv preprint arXiv:1603.08983},
  year    = {2016},
  doi     = {10.48550/arXiv.1603.08983}
}

@inproceedings{wang2018skipnet,
  title     = {SkipNet: Learning Dynamic Routing in Convolutional Networks},
  author    = {Wang, Lan and others},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year      = {2018}
}

@inproceedings{xin2020deebert,
  title     = {DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  author    = {Xin, Ji and Tang, Raphael and Lee, Jaejun and Yu, Yaoliang and Lin, Jimmy},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2020},
  doi       = {10.18653/v1/2020.acl-main.204}
}

@article{teerapittayanon2017branchynet,
  title   = {BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks},
  author  = {Teerapittayanon, Surat and McDanel, Bradley and Kung, H. T.},
  journal = {arXiv preprint arXiv:1709.01686},
  year    = {2017},
  doi     = {10.48550/arXiv.1709.01686}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% System-level Efficient VLM Inference
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{shao2025twigvlm,
  title     = {Growing a Twig to Accelerate Large Vision-Language Models},
  author    = {Shao, Zhenwei and Wang, Mingyang and Yu, Zhou and Pan, Wenwen and Yang, Yan and Wei, Tao and Zhang, Hongyuan and Mao, Ning and Chen, Wei and Yu, Jun},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Uncertainty / Selective Prediction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{geifman2017selective,
  title   = {Selective Classification for Deep Neural Networks},
  author  = {Geifman, Yonatan and El-Yaniv, Ran},
  journal = {arXiv preprint arXiv:1705.08500},
  year    = {2017},
  doi     = {10.48550/arXiv.1705.08500}
}

@inproceedings{geifman2019selectivenet,
  title     = {SelectiveNet: A Deep Neural Network with an Integrated Reject Option},
  author    = {Geifman, Yonatan and El-Yaniv, Ran},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2019}
}
