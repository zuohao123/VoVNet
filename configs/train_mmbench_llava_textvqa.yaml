# Training on MMBench dev + LLaVA-Instruct + TextVQA.

data:
  train_jsonl: "data/processed/mmbench_llava_textvqa/mmbench_llava_textvqa_train.jsonl"
  eval_jsonl: "data/processed/mmbench/mmbench_test.jsonl"
  prompt_template: "Question: {question}\nAnswer:"

model:
  torch_dtype: "float16"

training:
  epochs: 2
  stage1_epochs: 1
  stage1_baseline_name: "always_full"
  stage1_lambda_cost: 0.0
  per_device_batch_size: 1
  gradient_accumulation: 4
  log_every: 100
  mixed_precision: "fp16"
  gradient_checkpointing: true
  deepspeed_stage: 2
