# Two-stage training (2k + 5k) with per-sample prompts and MC letter targets.
data:
  train_jsonl: "data/processed/mmbench_llava_textvqa/mmbench_llava_textvqa_train_with_prompt_norm_relabeled.jsonl"
  eval_jsonl: "data/processed/mmbench/mmbench_dev_text_prompt.jsonl"
  prompt_template: "Question: {question}\nAnswer:"

model:
  torch_dtype: "float16"
  use_thinking_for_full: false

training:
  output_dir: "outputs/train_mmbench_llava_textvqa_vovnet_v3_2k5k"
  epochs: 8
  stage1_epochs: 1
  stage1_baseline_name: "always_full"
  stage1_lambda_cost: 0.0
  stage1_max_steps: 1000
  stage2_max_steps: 20001
  per_device_batch_size: 1
  gradient_accumulation: 4
  log_every: 100
  summary_every: 100
  mixed_precision: "fp16"
  gradient_checkpointing: true
  lr: 5e-5
  warmup_steps: 0
  max_grad_norm: 1.0
  save_every: 2000

policy:
  policy_mode: "logits"
  gain_supervision: false
  gain_loss_type: "rank_hinge"
  gain_loss_weight: 0.0
  gain_margin: 0.2
  policy_target_mode: "loss_margin"
  policy_ce_weight: 1.0
  policy_ce_weight_start: 1.0
  policy_ce_weight_end: 3.0
  policy_ce_weight_warmup_steps: 10000
  policy_delta_start: 0.0
  policy_delta_end: 0.10
  policy_delta_warmup_steps: 1000
  # Warmed deltas to avoid early collapse and keep N/C/F active.
  policy_delta_no_start: 0.02
  policy_delta_no_end: 0.05
  policy_delta_no_warmup_steps: 1000
  policy_delta_coarse_start: 0.02
  policy_delta_coarse_end: 0.05
  policy_delta_coarse_warmup_steps: 1000
  policy_guard_window: 2048
  policy_min_full_ratio: 0.06
  policy_min_full_warmup_steps: 6000
  policy_min_coarse_ratio: 0.12
  policy_min_coarse_warmup_steps: 6000
  policy_no_bias_start: 0.0
  policy_no_bias_end: 0.02
  policy_no_bias_warmup_steps: 5000
  policy_open_enable: true
  policy_open_quantile: 0.5
  policy_open_quantile_start: 0.0
  policy_open_quantile_end: 0.5
  policy_open_quantile_warmup_steps: 5000
  policy_open_margin: 0.02
  policy_open_use_best_vis: true
  policy_open_force_visual_warmup_steps: 3000
  policy_open_force_visual_action: "best_vis"
  policy_open_visual_bias_start: 0.02
  policy_open_visual_bias_end: 0.0
  policy_open_visual_bias_warmup_steps: 12000
  enable_soft_targets: true
  soft_target_temperature: 1.0
  lambda_cost: 0.0005
  cost_scale: 0.05
  cost_warmup_steps: 2000
  gumbel_tau: 1.5
  use_straight_through: true
  entropy_weight: 0.05
  entropy_weight_start: 0.05
  entropy_weight_end: 0.02
  entropy_weight_warmup_steps: 10000
  explore_prob: 0.2

vision_budget:
  coarse_long_side: 224
  full_long_side: 448
  coarse_max_pixels: 50176
  full_max_pixels: 200704
  patch_size: 14
  token_cap: null
